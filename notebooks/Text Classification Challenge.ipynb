{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Challenge\n",
    "\n",
    "Use python code to create a text classification model that uses a tf-idf feature set to\n",
    "classify documents in the Twenty News Groups Data Set (https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups).\n",
    "\n",
    "- Do not use any pre-built packages to generate the tf-idf features (i.e. you can use numpy, pandas etc. but not scikit)\n",
    "- Choose 3 different classification algorithms and compare how each of them does on the task\n",
    "- Of the 3 you test out, choose the one that performs the best for your final code\n",
    "- Provide a brief (less than 300 words) write-up explaining the following points:\n",
    "  - How well does your model perform at the given task?\n",
    "  - How did you compare the different classification algorithms?\n",
    "  - What are some of the limitations of the model?\n",
    "\n",
    "Publish your code to a private GitHub repo and send us an invitation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import metrics\n",
    "\n",
    "# We need a progress bar because this term-frequency \n",
    "# stuff is probably going to take a while.\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring docs into memory and strip out the metadata, which happens to be everything up to the first double newline characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = [group for group in os.listdir('../data/20_newsgroups') if group[0] != '.']\n",
    "len(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 20 groups, but we'll just pick a few to work with mainly due to memory and time constraints:\n",
    "\n",
    "- alt.atheism\n",
    "- comp.graphics\n",
    "- misc.forsale\n",
    "- sci.crypt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = ['alt.atheism', 'comp.graphics', 'misc.forsale', 'rec.autos', 'sci.crypt']\n",
    "\n",
    "# create a dictionary of all documents, preserving the name of the group \n",
    "# in the key\n",
    "docs_per_group = dict()\n",
    "all_docs = dict()\n",
    "\n",
    "for group in groups:\n",
    "    docs = os.listdir('../data/20_newsgroups/' + group)\n",
    "    docs_per_group[group] = len(docs)\n",
    "    for doc in docs:\n",
    "        doc_content = ''\n",
    "        for line in open(os.path.join('../data/20_newsgroups/', group, doc), encoding = \"ISO-8859-1\"):\n",
    "            doc_content += line\n",
    "\n",
    "        # for every doc, strip out the metadata, which is up to \n",
    "        # the first 2 newline characters in a row\n",
    "        doc_content = ''.join(doc_content.split('\\n\\n')[1:])\n",
    "\n",
    "        if doc_content != '':\n",
    "            doc_name = group + '_' + doc\n",
    "            all_docs[doc_name] = doc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4998"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the balance of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 1000,\n",
       " 'comp.graphics': 1000,\n",
       " 'misc.forsale': 1000,\n",
       " 'rec.autos': 1000,\n",
       " 'sci.crypt': 1000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_per_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the order of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_doc_names = list(all_docs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Train/Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the random `80:20` `train:test` splits and vectorize the target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It's reasonable for the model to not be aware of the vocabulary that's in the test set, so when extracting the features from the test set, only use the vocabulary set built up by the training data. There will be tokens that will not be used for creating features during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_DOCS = len(all_doc_names)\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "# split documents into 80/20\n",
    "train_doc_indices = np.random.choice(NUM_DOCS, np.floor(NUM_DOCS * TRAIN_SPLIT).astype(int), replace=False)\n",
    "test_doc_indices = np.delete(np.arange(NUM_DOCS), train_doc_indices) # take the remainder\n",
    "\n",
    "# vectorize targets\n",
    "string_targets = np.asarray([doc.split('_')[0] for doc in all_doc_names])\n",
    "uniques, vectorized_targets = np.unique(string_targets, return_inverse=True)\n",
    "\n",
    "vectorized_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_targets = []\n",
    "for i in train_doc_indices:\n",
    "    train_targets.append(vectorized_targets[i])\n",
    "\n",
    "train_targets = np.asarray(train_targets)\n",
    "    \n",
    "test_targets = []\n",
    "for i in test_doc_indices:\n",
    "    test_targets.append(vectorized_targets[i])\n",
    "    \n",
    "test_targets = np.asarray(test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alt.atheism', 'comp.graphics', 'misc.forsale', 'rec.autos',\n",
       "       'sci.crypt'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TF-IDF Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), The simplest form of term frequency is calculated using the \"raw count of a term in a document\", or $tf(t, d)=f_{t,d}$. If the term frequency is adjusted for document length, $tf(t, d)=f_{t,d} /$ `number of words in d`.\n",
    "\n",
    "The inverse document frequency is calculated by using $idf(t, D)=\\log(\\frac{N}{1 + |\\{d \\in D : t \\in d\\}|})$, where $N$ is the total number of documents in the corpus, and $|\\{d \\in D : t \\in d\\}|$ is the number of documents where the term t appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in addition to standard stopwords by nltk, want to remove punctuation\n",
    "stop_words = stopwords.words('english') + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function for preprocessing each document, which includes:\n",
    "- tokenization\n",
    "- lowercasing\n",
    "- stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_doc(doc, stop_words):\n",
    "    \"\"\"\n",
    "    Remove all tokens with digits and stopwords.\n",
    "    \"\"\"\n",
    "    preprocessed_tokens = []\n",
    "    tokens = word_tokenize(doc)\n",
    "    for token in tokens:\n",
    "        token = re.sub('[\\W_]', '', token.lower())\n",
    "        if token not in stop_words and token != '' and not token.isdigit():\n",
    "            preprocessed_tokens.append(token)\n",
    "    return preprocessed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up three functions below, which all rely on each other for calculating the overall tf-idf feature for each document. Each document's tf-idf features will be mapped to a target, which is the type of document (one of the groups that we chose: `alt.atheism`, `comp.graphics`, `misc.forsale`, or `sci.crypt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_tf(token, doc):\n",
    "    \"\"\"\n",
    "    (str, list) --> float\n",
    "    \n",
    "    Calculate the token's term frequency.\n",
    "    \n",
    "    This function assumes that the contents of the doc have been tokenized\n",
    "    \"\"\"\n",
    "    return float(doc.count(token) / (len(doc) + 1))\n",
    "\n",
    "def calculate_idf(array, num_docs):\n",
    "    \"\"\"\n",
    "    (np.array, int) --> np.array\n",
    "    \n",
    "    Calculate the inverse document frequency, which is:\n",
    "    the log of the number of documents in the corpus divided\n",
    "    by the number of docs where the term appears\n",
    "    \n",
    "    the term_index in the matrix is given, the sum of which \n",
    "    gives the number of docs in which the term appears\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.log(num_docs / (array + 1).astype(float))\n",
    "\n",
    "def calculate_tf_idf(token, doc, idf_array, vocabulary_index):\n",
    "    \"\"\"\n",
    "    (str, list, np.array, dict)\n",
    "    \n",
    "    Calculates the term frequency-inverse document frequency.\n",
    "    \n",
    "    \"\"\"\n",
    "    if token not in vocabulary_index:\n",
    "        return float(0)\n",
    "    else:\n",
    "        return calculate_tf(token, doc) * idf_array[vocabulary_index[token]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the feature sets for train/test, \n",
    "# removing all non-alphanumeric characters, digits and stopwords\n",
    "\n",
    "all_train_tokens = set()\n",
    "all_test_tokens = set()\n",
    "\n",
    "for i in train_doc_indices:\n",
    "    tokens = preprocess_doc(all_docs[all_doc_names[i]], stop_words)\n",
    "    all_train_tokens.update(tokens)\n",
    "\n",
    "for j in test_doc_indices:\n",
    "    tokens = preprocess_doc(all_docs[all_doc_names[j]], stop_words)\n",
    "    all_test_tokens.update(tokens)\n",
    "    \n",
    "all_train_tokens = list(all_train_tokens)\n",
    "all_test_tokens = list(all_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51881"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23333"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create vocabulary lookup dictionaries\n",
    "# only for training, because test vocabulary is not known!\n",
    "\n",
    "train_vocabulary_index = dict()\n",
    "\n",
    "i = 0\n",
    "for token in all_train_tokens:\n",
    "    train_vocabulary_index[token] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51881"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocabulary_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the inverse document frequency arrays for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3998/3998 [00:13<00:00, 307.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train_idf_array = np.zeros(len(all_train_tokens))\n",
    "\n",
    "# train\n",
    "for i in tqdm(train_doc_indices):\n",
    "    preprocessed_tokens = set(preprocess_doc(all_docs[all_doc_names[i]], stop_words))\n",
    "    for token in preprocessed_tokens:\n",
    "        train_idf_array[train_vocabulary_index[token]] += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 286.62it/s]\n"
     ]
    }
   ],
   "source": [
    "test_idf_array = np.zeros(len(all_train_tokens))\n",
    "\n",
    "keyerrors = 0\n",
    "# test\n",
    "for i in tqdm(test_doc_indices):\n",
    "    preprocessed_tokens = set(preprocess_doc(all_docs[all_doc_names[i]], stop_words))\n",
    "    for token in preprocessed_tokens:\n",
    "        try:\n",
    "            test_idf_array[train_vocabulary_index[token]] += 1.0\n",
    "        except KeyError:\n",
    "            keyerrors += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tokens that were in the test set but not in the training set are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6577"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyerrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idf_array = calculate_idf(train_idf_array, len(train_doc_indices))\n",
    "test_idf_array = calculate_idf(test_idf_array, len(test_doc_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every document and for every token, calculate the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3998/3998 [11:49<00:00,  6.33it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tf_idf_matrix = np.zeros(shape=(len(train_doc_indices), len(all_train_tokens)))\n",
    "\n",
    "# train\n",
    "i = 0\n",
    "for index in tqdm(train_doc_indices):\n",
    "    tokenized_doc = list(preprocess_doc(all_docs[all_doc_names[index]], stop_words))\n",
    "    for j in range(len(all_train_tokens)):\n",
    "        train_tf_idf_matrix[i][j] = calculate_tf_idf(all_train_tokens[j], tokenized_doc, train_idf_array, train_vocabulary_index)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:19<00:00, 12.63it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tf_idf_matrix = np.zeros(shape=(len(test_doc_indices), len(all_train_tokens)))\n",
    "\n",
    "test_tf_idf_keyerrors = 0\n",
    "\n",
    "# test\n",
    "i = 0\n",
    "for index in tqdm(test_doc_indices):\n",
    "    tokenized_doc = list(preprocess_doc(all_docs[all_doc_names[index]], stop_words))\n",
    "    for j in range(len(all_test_tokens)):\n",
    "        try:\n",
    "            test_tf_idf_matrix[i][train_vocabulary_index[all_test_tokens[j]]] = calculate_tf_idf(all_test_tokens[j], tokenized_doc, test_idf_array, train_vocabulary_index)\n",
    "        except KeyError:\n",
    "            test_tf_idf_keyerrors += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6206000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tf_idf_keyerrors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the shapes of the matrices match up with the train and test tf-idf matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998, 51881)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tf_idf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 51881)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tf_idf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I chose 3 models to test the classification of text:\n",
    "- Naive Bayes\n",
    "- SVM\n",
    "- KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the simplest of them all, the Naive Bayes classifier is \"naive\" because of the assumption that all of the features in a dataset are mutually independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95699999999999996"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_clf = MultinomialNB().fit(train_tf_idf_matrix, train_targets)\n",
    "nb_predicted = nb_clf.predict(test_tf_idf_matrix)\n",
    "np.mean(nb_predicted == test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Precision**: % of selected items that are correct\n",
    "- **Recall**: % of correct items that are selected\n",
    "- **F1-score**: the harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.98      0.97      0.97       213\n",
      "comp.graphics       0.94      0.95      0.95       211\n",
      " misc.forsale       0.96      0.93      0.94       197\n",
      "    rec.autos       0.94      0.97      0.95       182\n",
      "    sci.crypt       0.97      0.96      0.97       197\n",
      "\n",
      "  avg / total       0.96      0.96      0.96      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_targets, nb_predicted, target_names=uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[206,   2,   0,   2,   3],\n",
       "       [  2, 201,   6,   1,   1],\n",
       "       [  0,   4, 183,   9,   1],\n",
       "       [  0,   3,   1, 177,   1],\n",
       "       [  2,   4,   1,   0, 190]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_targets, nb_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94999999999999996"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "svm_clf.fit(train_tf_idf_matrix, train_targets)\n",
    "svm_predicted = svm_clf.predict(test_tf_idf_matrix)\n",
    "np.mean(svm_predicted == test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.99      0.96      0.97       213\n",
      "comp.graphics       0.88      0.96      0.92       211\n",
      " misc.forsale       0.93      0.94      0.94       197\n",
      "    rec.autos       0.98      0.94      0.96       182\n",
      "    sci.crypt       0.98      0.94      0.96       197\n",
      "\n",
      "  avg / total       0.95      0.95      0.95      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_targets, svm_predicted, target_names=uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[205,   6,   0,   1,   1],\n",
       "       [  2, 202,   6,   1,   0],\n",
       "       [  0,   9, 186,   1,   1],\n",
       "       [  0,   5,   5, 171,   1],\n",
       "       [  1,   8,   2,   0, 186]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_targets, svm_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34100000000000003"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(train_tf_idf_matrix, train_targets)\n",
    "knn_predicted = knn_clf.predict(test_tf_idf_matrix)\n",
    "np.mean(knn_predicted == test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.95      0.24      0.39       213\n",
      "comp.graphics       0.82      0.26      0.39       211\n",
      " misc.forsale       0.23      0.99      0.38       197\n",
      "    rec.autos       1.00      0.13      0.22       182\n",
      "    sci.crypt       1.00      0.08      0.15       197\n",
      "\n",
      "  avg / total       0.80      0.34      0.31      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_targets, knn_predicted, target_names=uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 52,   3, 158,   0,   0],\n",
       "       [  2,  54, 155,   0,   0],\n",
       "       [  0,   1, 196,   0,   0],\n",
       "       [  0,   4, 155,  23,   0],\n",
       "       [  1,   4, 176,   0,  16]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_targets, knn_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN is very slow and not accurate at all for its current parameter setting of 5 neighbors. I don't think it's worth the time trying to do any sort of grid search because the naive bayes and SVM perform so well out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model that performed the best was the multinomial Naive Bayes classifier, receiving an accuracy score of 95.7%. I think that the large part of the performance boost came from the initial feature extraction, which included preprocessing steps such as stopword and punctuation removal, as well as the fact that I had started off with a balanced dataset. \n",
    "\n",
    "The different classification algorithms were compared using their overall accuracy, precision, recall, F1 scores and through a visual perusal of the confusion matrix. The multinomial Naive Bayes classifier's precision, recall and F1-scores were all 96%. \n",
    "\n",
    "The main limitation of the Naive Bayes classifier is its primary assumption that all of the data that it models are independent and identically distributed random variables, which would mean that it is unable to model data that violates this assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future To-Do\n",
    "\n",
    "- proper cross validation (k-fold)\n",
    "- prettier confusion matrices with `pandas_ml`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
